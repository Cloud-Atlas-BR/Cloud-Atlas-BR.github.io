---
layout: post
title: Captura de logs com Sagemaker Endpoints
subtitle: ML Drops v3
tags: [aws, mlops, sagemaker, deploy, log]
comments: true
draft: true
---

{: .box-note}
**Este é um texto em desenvolvimento**: Ainda estamos escrevendo e/ou revisando seu conteúdo. Até o dia de sua publicação, ele não estará listado na página inicial do blog.

Se engana o cientista de dados que acredita que seu trabalho está finalizado com o treinamento de um modelo, assim como o engenheiro de ML quando sobe o modelo para produção. O ciclo de vida de um modelo de machine learning é, literalmente, sem fim.

<center>
<blockquote class="imgur-embed-pub" lang="en" data-id="a/RBWQRTj"  ><a href="//imgur.com/a/RBWQRTj">O ciclo sem fim do Machine Learning</a></blockquote><script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script>
</center>

Seja no início, no fim ou no meio, dados são uma importante força para esse ciclo continuar girando. 

Novas fontes de dados motivam a experimentação de diferentes abordagens por parte dos cientistas de dados, informações do mercado geram novos requisitos por parte das áreas de negócio e o **log dos modelos em produção orienta estratégias de recalibração, alarmes e alertas**.

Acompanhar o comportamento real do seu modelo, recebendo chamadas de sistemas e pessoas, é peça indispensável para que o ciclo de vida de um modelo seja **saudável**.

**Conhecer os consumidores do seu modelo** evita que o seu *output* seja consumido por outros sistemas sem que exista uma governança estabelecida. Isto também permite que estes consumidores possam ser alertados sobre mudanças com potencial geração de incidentes.

**Conhecer os dados que são enviados para seu modelo** para obtenção de predições nos permite avaliar se ele está sendo consumido para o uso que foi desenvolvido, e se ainda pode ser utilizado mesmo com alterações quantitativas e qualitativas dos dados ao longo do tempo.

Finalmente, **conhecer as respostas que seu modelo gera** nos dá insumos para avaliar enviesamentos, interpretar resultados e garantir que o modelo se comporta dentro de sua performance esperada.

Ufa!

Não faltam motivos para registrar e analisar os logs de seu modelo. Contudo, criar e manter a infraestrutura responsável por tal pode ser custoso.

Por esta razão, a AWS fornece uma conveniente *feature* junto com o serviço [Sagemaker Endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html): *Data capture*.

## Sagemaker Endpoints

Os Sagemaker Endpoints são a solução de deploy de modelos online de Machine Learning da AWS. De forma resumida, se trata de um container Docker que expõe seu modelo na forma de um REST API. Toda a infraestrutura é gerenciada pela AWS, 

<p style="text-align: center"><a href="https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/"><img src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/05/17/load-test-sagemaker-5.gifv"></a></p>

