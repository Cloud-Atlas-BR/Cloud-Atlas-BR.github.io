---
layout: post
title: Captura de logs com Sagemaker Endpoints
subtitle: ML Drops v3
tags: [aws, mlops, sagemaker, deploy, log]
comments: true
draft: false
---

Se engana o cientista de dados que acredita que seu trabalho está finalizado com o treinamento de um modelo, assim como o engenheiro de ML quando sobe o modelo para produção. O ciclo de vida de um modelo de machine learning pode ser, literalmente, sem fim.

<center>
<blockquote class="imgur-embed-pub" lang="en" data-id="a/RBWQRTj"  ><a href="//imgur.com/a/RBWQRTj">O ciclo sem fim do Machine Learning</a></blockquote><script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script>
</center>

Seja no início, no fim ou no meio, dados são uma importante força para esse ciclo continuar girando. 

Novas fontes de dados motivam a experimentação de diferentes abordagens por parte dos cientistas de dados, informações do mercado geram novos requisitos por parte das áreas de negócio e o **log dos modelos em produção orienta estratégias de recalibração, alarmes e alertas**.

Acompanhar o comportamento real do seu modelo, recebendo chamadas de sistemas e pessoas, é peça indispensável para que o ciclo de vida de um modelo seja **saudável**.

**Conhecer os consumidores do seu modelo** evita que o seu *output* seja consumido por outros sistemas sem que exista uma governança estabelecida. Isto também permite que estes consumidores possam ser alertados sobre mudanças com potencial geração de incidentes.

**Conhecer os dados que são enviados para seu modelo** para obtenção de predições nos permite avaliar se ele está sendo consumido para o uso que foi desenvolvido, e se ainda pode ser utilizado mesmo com alterações quantitativas e qualitativas dos dados ao longo do tempo.

Finalmente, **conhecer as respostas que seu modelo gera** nos dá insumos para avaliar enviesamentos, interpretar resultados e garantir que o modelo se comporta dentro de sua performance esperada.

Ufa!

Não faltam motivos para registrar e analisar os logs de seu modelo. Contudo, criar e manter a infraestrutura responsável por tal pode ser custoso.

Por esta razão, a AWS fornece uma conveniente *feature* junto com o serviço [Sagemaker Endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html): *Data capture*.

## Sagemaker Endpoints

Os Sagemaker Endpoints são a solução de deploy de modelos online de Machine Learning da AWS. De forma resumida, se trata de um container Docker que expõe seu modelo na forma de um REST API. Toda a infraestrutura é gerenciada pela AWS, ficando a configuração do deploy sob responsabilidade da engenharia/ciência de dados.

<p style="text-align: center"><a href="https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/"><img src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/05/17/load-test-sagemaker-3.gif"></a></p>

Para exemplificar o registro de logs dos Sagemaker Endpoints via *Data capture*, vamos criar e implantar um modelo de classificação a partir do *dataset* `iris`.

```python
import pandas as pd
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True, as_frame=True)
dados = pd.concat([y, X], axis=1)
dados.to_csv("data/dados.csv", header=False, index=False)
```

Acima, carregamos os dados e escrevemos em disco o arquivo *csv* no formato esperado pelo Sagemaker.

Em seguida, enviamos os dados para um bucket S3 para uso do Sagemaker no treinamento do modelo.

```python
import sagemaker

bucket = sagemaker.Session().default_bucket()
input_data = sagemaker.Session().upload_data(path="data", bucket=bucket)
train_input = TrainingInput(input_data, content_type="csv")
```

Configuramos o treinamento de um modelo de XGBoost gerenciado pela AWS com as seguintes linhas de código.

```python
import sagemaker
from sagemaker.estimator import Estimator

xgboost_container = sagemaker.image_uris.retrieve("xgboost", "us-east-1", "1.2-1")
role = sagemaker.get_execution_role() # Uma role que o Sagemaker possa assumir

hyperparameters = {
    "objective":"multi:softmax",
    "num_round":"2",
    "num_class": "3"
}

estimator = Estimator(image_uri=xgboost_container,
                      role=role,
                      hyperparameters=hyperparameters,
                      instance_count=1,
                      instance_type='ml.m5.2xlarge', 
                      volume_size=5,
                      output_path=f"s3://{bucket}")
```

Finalmente, treinamos nosso modelo. 

```python
estimator.fit({'train': train_input})
```

Um retorno semelhante ao apresentado abaixo será apresentado em caso de sucesso.

```
2021-03-09 02:53:32 Starting - Starting the training job...
2021-03-09 02:53:56 Starting - Launching requested ML instancesProfilerReport-1615258411: InProgress
......
2021-03-09 02:54:57 Starting - Preparing the instances for training...
2021-03-09 02:55:37 Downloading - Downloading input data...
2021-03-09 02:55:57 Training - Downloading the training image......
INFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)
INFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode
INFO:root:Determined delimiter of CSV input is ','
INFO:root:Single node training.
INFO:root:Train matrix has 150 rows
[0]#011train-merror:0.04000
[1]#011train-merror:0.04000

2021-03-09 02:57:39 Completed - Training job completed
ProfilerReport-1615258411: NoIssuesFound
Training seconds: 120
Billable seconds: 120
```

A partir do objeto `estimator` podemos realizar o deploy de um sagemaker endpoint, e faremos isso adicionando a configuração de *Data Capture*.

```python
from sagemaker.model_monitor import DataCaptureConfig

data_capture_config = DataCaptureConfig(
    enable_capture=True,
    sampling_percentage=100,
    destination_s3_uri=f"s3://{bucket}",
    capture_options=["REQUEST", "RESPONSE"]
)
```

Na configuração acima, informamos que iremos capturar tanto a requisição quanto a resposta do modelo para 100% das chamadas do modelo, e salvaremos essas informações em um Bucket do S3.

Chamando o método `deploy` do objeto `estimator` colocamos nosso modelo em produção.

```python
estimator.deploy(initial_instance_count=1,
                 instance_type="ml.t2.medium", 
                 data_capture_config = data_capture_config)
```

Assim que o provisionamento do Sagemaker Endpoint ser finalizado, experimente fazer uma chamada ao modelo da seguinte forma.

```python
import boto3

features = pd.DataFrame(X)
features.to_csv("features.csv", header=False, index=False)
request = open("features.csv").read().splitlines()[0]

client = boto3.client("sagemaker-runtime")

response = client.invoke_endpoint(EndpointName="nome_do_endpoint",
                                  Body=request)

predicao = reponse["Body"].read()

'0.0'
```

Com nossa primeira predição realizada, vamos conferir os logs dela em nosso bucket.

<p style="text-align: center"><img src="https://i.imgur.com/afu1ebr.png"></p>

O bucket contém o diretório `dados/` com os dados que enviamos para treinamento e um diretório com o nome do nosso Sagemaker Endpoint. 

Ao acessar este segundo diretório, veremos uma estrutura particular para a organização desses logs:

`Bucket > Endpoint > Modelo > Ano > Mês > Dia > Hora`

<p style="text-align: center"><img src="https://i.imgur.com/IacgFjb.png"></p>

Esse padrão de diretório não é aleatório. Esta é a forma de particionamento de dados que a AWS utiliza/recomenda para seus produtos de Analytics (e.g. Kinesis, Athena).

Os logs estão salvos no formato [.jsonl](https://jsonlines.org/) com um nome único, e cada arquivo contém informações de uma ou mais chamadas ao Sagemaker Endpoint.

Observe abaixo o conteúdo do log de uma requisição realizada para o Sagemaker Endpoint.

```json
{
    "captureData": {
        "endpointInput": {
            "observedContentType": "csv",
            "mode": "INPUT",
            "data": "Ni44LDIuOCw0LjgsMS40",
            "encoding": "BASE64"
        },
        "endpointOutput": {
            "observedContentType": "text/csv; charset=utf-8",
            "mode": "OUTPUT",
            "data": "1.0",
            "encoding": "CSV"
        }
    },
    "eventMetadata": {
        "eventId": "c9f7ffce-84ea-4b7a-90ed-f8b3f38dd830",
        "inferenceTime": "2021-03-09T03:38:10Z"
    },
    "eventVersion": "0"
}
```

De forma **completamente gerenciada** pela AWS, obtemos os dados enviados na requisição (em base64), a resposta do modelo, um identificador único daquela requisição, e o horário da requisição.

Estes logs podem ser alimentados em outros sistemas para definir a necessidade de recalibração, geração de alertas ou mesmo para processos de auditória. A análise desses dados também pode sensibilizar os times de ciência de dados para a melhoria desses modelos.

Dependendo da necessidade, é possível configurar o *Data Capture* para registrar o log de apenas um percentual das chamadas feitas para o Sagemaker Endpoint, salvar apenas os dados da requisição ou apenas a resposta do modelo. 

Também é possível manter, no log, informações customizadas pelo cliente. Para tal, basta que as chamadas ao Sagemaker Endpoint pelo método `invoke_endpoint` possuam o parâmetro `CustomAttributes`.

## Pensamentos finais

O Sagemaker Endpoint se destaca como alternativa de deploy de Machine Learning por conta de seu ecossistema de serviços e APIs, o *Data Capture* é um exemplo vivo deste fato.

Ao associar um baixo custo de armazenamento desses logs através do S3 e gerenciar completamente a captura deles, a AWS fornece peça essencial para a evolução contínua de seu modelo.

Até o próximo Drops!

## Referências

* [Amazon SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)