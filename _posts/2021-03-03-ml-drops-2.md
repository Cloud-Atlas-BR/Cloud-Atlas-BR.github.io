---
layout: post
title: Reutilização de contexto de Funções Lambda
subtitle: ML Drops v2
tags: [aws, mlops, cloudformation, iaac, iac]
comments: true
draft: false
---

Fala Galera!

No Drops de hoje, vamos falar sobre um assunto bem interessante, que apesar de não ser novo, muitas vezes passa desapercebido durante o desenvolvimento de aplicações *serverless*: **A reutilização de contexto das funções Lambda**.

Quando falamos de arquiteturas *serverless* na AWS, as [funções Lambda](https://aws.amazon.com/pt/Lambda/) sempre aparecem como forte candidata de uso devido ao grande poder de paralelismo, escalonamento e também alta disponibilidade. 

O *pricing* é um ponto de destaque. Além de garantir um [nível gratuito](https://aws.amazon.com/Lambda/pricing/) de 1 milhão de requisições e 400.000 GB.s de computação por mês, somos cobrados por milissegundo de execução.

Mesmo com tantos benefícios, entender como as funções Lambda são instanciadas e otimizadas pela AWS pode fornecer ganhos em todos os pontos abordados acima.

## Instanciamento e Spawn

Quando executamos uma função Lambda, pedimos de forma explicita para que a AWS nos disponibilize uma infraestrutura e também um container (gerenciado) que será responsável pela execução de nosso código.

Como falamos anteriormente, um dos benefícios de uma função Lambda é o seu poder de escalabilidade. Caso enviássemos múltiplas requisições a uma mesma função Lambda, a AWS responderia criando diversas cópias do container contendo nosso código para garantir que todas as requisições possam ser processadas em paralelo.

<p style="text-align: center"><img src="https://i.imgur.com/YIINDjE.png"></p>

É comum possuirmos conexões a Bancos de Dados (RDS), buckets S3, filas SQS e outros serviços dentro no código de uma função Lambda. Tais conexões aumentam o tempo de execução das nossas funções, uma vez que as mesmas tem como objetivo fornecer uma experiência [stateful](https://nordicapis.com/defining-stateful-vs-stateless-web-services/).

Com isso, ao executarmos uma função Lambda pela primeira vez, ou então, executarmos a mesma função depois de um período de inatividade, nos deparamos com o que a AWS chama de [cold start](https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/).

## Cold Start e Reutilização de contexto

As etapas do processo de `cold start` de funções Lambda são:

**(1)** Download de dependências e do código da função Lambda  
**(2)** Inicialização do container  
**(3)** Execução do código da função Lambda antes do *handler*  
**(4)** Execução do *handler* da função Lambda  
  
O fluxo acima sempre irá ocorrer na primeira execução de uma função Lambda, porém, a medida que as funções são executadas de forma escalonada e paralelizada, a AWS prioriza a **otimização** dos seus recursos.

Ou seja, porque não reutilizarmos funções Lambda já instanciadas em novas requisições?

É essa otimização que a AWS realiza, a chamada **reutilização de contexto**.

Quando reutilizamos containers já instanciados, pulamos diretamente para o **quarto** passo do processo de `cold start`, processo conhecido como `warm start`.
  
O uso de `warm start` pode ser configurado diretamente no código fonte de nossa função Lambda, garatindo que conexões/integrações que ocorreram durante o `cold start` sejam reutilizadas (e.g. carregamento de .pkl, ou a conexão da base dados).

Para isso, basta que as conexões/integrações estejam escritas **fora do método handler**, assim os contextos criados serão reutilizadas em vez de instanciados novamente.

Com esta modificação, garantimos uma redução no tempo de execução da nossa função sempre que o processo de **reutilização de contexto** estiver ativo. Por padrão, cada função Lambda permanece em `warm start` por 15 minutos após a primeira execução.

<p style="text-align: center"><img src="https://i.imgur.com/LUPN0uE.png"></p>

## Bora pro Código

Vamos criar uma função Lambda com o seguinte código.

```python
import json
import boto3

def handler(event, context):
    
    s3 = boto3.client("s3")
    buckets = s3.list_buckets()
    
    return {
        'statusCode': 200,
        'body': json.dumps(buckets, default=str) 
    }
```

No código acima criamos uma conexão com o serviço S3 via boto3 e listamos os buckets S3 em nossa conta.

Ao fazer a primeira chamada dessa função Lambda, levou-se aproximadamente ~1,5 segundos. Já a segunda chamada durou ~400 ms. Um exemplo claro dos comportamentos de `cold start` e `warm start`.

<p style="text-align: center;margin-bottom:0"><img src="https://i.imgur.com/1UZSE23.png"></p>
<p style="text-align: center; margin-top:0">Cold start da função Lambda com código dentro do handler</p>

<p style="text-align: center;margin-bottom:0"><img src="https://i.imgur.com/3R05WG8.png"></p>
<p style="text-align: center; margin-top:0">Warm start da função Lambda com código dentro do handler</p>

Agora, alterando o código da função para que o contexto de conexão ao S3 esteja **fora** do *handler*, chegamos no seguinte código.

```python
import json
import boto3

s3 = boto3.client("s3")
buckets = s3.list_buckets()

def Lambda_handler(event, context):
        
    return {
        'statusCode': 200,
        'body': json.dumps(buckets, default=str) 
    }
```

Os resultados são impressionantes! A segunda chamada da função Lambda, fazendo a conexão com o S3 fora do *handler*, durou apenas **1 ms**!!!

<p style="text-align: center;margin-bottom:0"><img src="https://i.imgur.com/i0Y2g0R.png"></p>
<p style="text-align: center; margin-top:0">Warm start da função Lambda com código fora do handler</p>

## Pensamentos finais

Conhecer as diferenças do `cold start` e do `warm start` fazem toda a diferença no uso de funções Lambda.

Pensando em termos de machine learning, onde modelos carregados podem ter tamanhos variando entre MB e GB, utilizar essa *feature* pode ser determinante na performance do deploy do seu modelo.

Até o próximo Drops!

## Referências

* [New for AWS Lambda – Predictable start-up times with Provisioned Concurrency](https://aws.amazon.com/blogs/compute/new-for-aws-lambda-predictable-start-up-times-with-provisioned-concurrency/)